{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1"
      ],
      "metadata": {
        "id": "XPtrh3L4IG_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Wrangling, I\n",
        "Perform the following operations using Python on any open source dataset (e.g., data.csv)\n",
        "1. Import all the required Python Libraries.\n",
        "2. Locate an open source data from the web (e.g., https://www.kaggle.com). Provide a clear\n",
        " description of the data and its source (i.e., URL of the web site).\n",
        "3. Load the Dataset into pandas dataframe.\n",
        "4. Data Preprocessing: check for missing values in the data using pandas isnull(), describe()\n",
        "function to get some initial statistics. Provide variable descriptions. Types of variables etc.\n",
        "Check the dimensions of the data frame.\n",
        "5. Data Formatting and Data Normalization: Summarize the types of variables by checking\n",
        "the data types (i.e., character, numeric, integer, factor, and logical) of the variables in the\n",
        "data set. If variables are not in the correct data type, apply proper type conversions.\n",
        "6. Turn categorical variables into quantitative variables in Python.\n",
        "In addition to the codes and outputs, explain every operation that you do in the above steps and\n",
        "explain everything that you do to import/read/scrape the data set.\n",
        "\n",
        "## Data Wrangling in Python: Exploring a Dataset\n",
        "\n",
        "**Concepts and Definitions:**\n",
        "\n",
        "- **Data Wrangling:** The process of transforming raw data into a clean and usable format for analysis. It involves tasks like data cleaning, missing value imputation, and formatting.\n",
        "- **Open-source Dataset:** Datasets publicly available for anyone to download and use.\n",
        "- **Pandas:** A powerful Python library for data analysis and manipulation.\n",
        "- **Data Preprocessing:** Preparing data before analysis, often including handling missing values, formatting data types, and outlier detection.\n",
        "- **Data Types (pandas):**\n",
        "   - **Numeric:** Numbers (e.g., integer, float)\n",
        "   - **Object:** Strings, text data\n",
        "   - **Categorical:** Discrete categories with no inherent order (e.g., \"Yes/No\", colors)\n",
        "   - **Boolean:** True/False values\n",
        "- **Data Normalization:** Scaling or transforming features to a common range to improve model performance in some machine learning tasks.\n",
        "\n",
        "**Example Using Kaggle Dataset**\n",
        "\n",
        "**1. Import Libraries:**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np  # May be needed for data type conversions\n",
        "```\n",
        "\n",
        "**2. Locate Open-source Data:**\n",
        "\n",
        "Let's use the \"Used Cars\" dataset from Kaggle: [https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data](https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data)\n",
        "\n",
        "This dataset contains information about used cars, including variables like mileage, price, model year, and manufacturer.\n",
        "\n",
        "**3. Load Dataset:**\n",
        "\n",
        "```python\n",
        "# Download the data from Kaggle (specific instructions may vary)\n",
        "# Assuming you have downloaded the data as \"used_cars.csv\"\n",
        "\n",
        "# Load data into a pandas DataFrame\n",
        "used_cars_data = pd.read_csv(\"used_cars.csv\")\n",
        "```\n",
        "\n",
        "**4. Data Preprocessing:**\n",
        "\n",
        "- **Check Missing Values:**\n",
        "\n",
        "```python\n",
        "# Check for missing values\n",
        "print(used_cars_data.isnull().sum())  # Shows the count of missing values per column\n",
        "```\n",
        "\n",
        "- **Describe Data:**\n",
        "\n",
        "```python\n",
        "# Get basic statistics\n",
        "print(used_cars_data.describe())  # Provides summary statistics for numeric columns\n",
        "```\n",
        "\n",
        "**Variable Descriptions:**\n",
        "\n",
        "The specific variable descriptions will depend on the dataset you choose. Here's an example for the \"Used Cars\" dataset:\n",
        "\n",
        "- `model_year`: Numeric (integer), represents the year the car was manufactured.\n",
        "- `mileage`: Numeric (integer/float), represents the car's odometer reading.\n",
        "- `price`: Numeric (float), represents the car's asking price.\n",
        "- `make`: Categorical (string), represents the car's manufacturer.\n",
        "- `model`: Categorical (string), represents the car's specific model.\n",
        "- (Other columns might exist)\n",
        "\n",
        "- **Dimensions:**\n",
        "\n",
        "```python\n",
        "# Get data frame dimensions (rows, columns)\n",
        "print(used_cars_data.shape)\n",
        "```\n",
        "\n",
        "**5. Data Formatting and Normalization:**\n",
        "\n",
        "- **Data Types:**\n",
        "\n",
        "```python\n",
        "# Check data types of all columns\n",
        "print(used_cars_data.dtypes)\n",
        "```\n",
        "\n",
        "- **Type Conversions (if necessary):**\n",
        "\n",
        "```python\n",
        "# Example: Convert 'mileage' to numeric (assuming it's currently an object)\n",
        "if used_cars_data['mileage'].dtype == 'object':\n",
        "  try:\n",
        "    # Attempt conversion to numeric (may require handling errors)\n",
        "    used_cars_data['mileage'] = pd.to_numeric(used_cars_data['mileage'], errors='coerce')  # Replace with appropriate error handling\n",
        "  except:\n",
        "    print(\"Error converting 'mileage' to numeric\")\n",
        "```\n",
        "\n",
        "**6. Turning Categorical Variables into Quantitative Variables:**\n",
        "\n",
        "There are several approaches for this, depending on the context of your analysis:\n",
        "\n",
        "- **One-Hot Encoding:** Creates new binary columns for each category, indicating presence/absence.\n",
        "\n",
        "```python\n",
        "# Example: One-hot encode 'make' (assuming it has multiple categories)\n",
        "make_dummies = pd.get_dummies(used_cars_data['make'], prefix='make_')\n",
        "used_cars_data = pd.concat([used_cars_data, make_dummies], axis=1)\n",
        "# Drop the original 'make' column if desired\n",
        "used_cars_data.drop('make', axis=1, inplace=True)\n",
        "```\n",
        "\n",
        "- **Label Encoding:** Assigns a numeric value (integer) to each category. This might not be ideal for all scenarios.\n",
        "\n",
        "```python\n",
        "# Example: Label encode 'model' (use with caution if order matters)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "used_cars_data['model_encoded'] = le.fit_transform(used_cars_data['model'])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1W0ccFo3ILom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "RaHHYP2SInoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2:Data Wrangling II\n",
        "Create an “Academic performance” dataset of students and perform the following operations using\n",
        "Python.\n",
        "1. Scan all variables for missing values and inconsistencies. If there are missing values and/or\n",
        "inconsistencies, use any of the suitable techniques to deal with them.\n",
        "2. Scan all numeric variables for outliers. If there are outliers, use any of the suitable\n",
        "techniques to deal with them.\n",
        "3. Apply data transformations on at least one of the variables. The purpose of this\n",
        "transformation should be one of the following reasons: to change the scale for better\n",
        "understanding of the variable, to convert a non-linear relation into a linear one, or to\n",
        "decrease the skewness and convert the distribution into a normal distribution.\n",
        "Reason and document your approach properly.\n",
        "\n",
        "## Data Wrangling for Academic Performance Data: Theory and Practice\n",
        "\n",
        "**Concepts and Definitions:**\n",
        "\n",
        "- **Data Wrangling:** The process of cleaning, transforming, and preparing data for analysis.\n",
        "- **Missing Values:** Data points that are absent or incomplete.\n",
        "- **Inconsistencies:** Inaccuracies, formatting errors, or unexpected values in the data.\n",
        "- **Outliers:** Data points that fall significantly outside the overall distribution.\n",
        "- **Data Transformation:** Modifying data to improve its usability for analysis. Reasons include:\n",
        "   - Scaling: Standardizing the range of values for better comparison.\n",
        "   - Linearization: Transforming non-linear relationships into linear ones.\n",
        "   - Normalization: Transforming distributions closer to a normal (bell-shaped) curve.\n",
        "\n",
        "**Techniques:**\n",
        "\n",
        "- **Missing Values:** Imputation (filling in missing values), deletion (removing rows/columns with missing data).\n",
        "- **Outliers:** Capping (setting a limit), winsorization (replacing with a threshold value), removal (if justified).\n",
        "- **Transformations:** Log transformation (compressing large values), square root transformation (reducing spread), standardization (z-score), normalization (min-max scaling).\n",
        "\n",
        "**Code Example (Illustrative):**\n",
        "\n",
        "```\n",
        "python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data (replace with your actual data source)\n",
        "data = {\n",
        "    \"Student ID\": [1001, 1002, 1003, 1004, 1005],\n",
        "    \"Name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\"],  # Missing value example\n",
        "    \"Age\": [18, 19, 20, 21, 16],  # Potential outlier (younger)\n",
        "    \"Math Score\": [85, 92, 78, 95, None],  # Missing value example\n",
        "    \"English Score\": [72, 88, 65, 82, 90]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Missing Values and Inconsistencies\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())  # Shows count of missing values per column\n",
        "\n",
        "# Handle missing values (example: imputation with mean)\n",
        "df[\"Math Score\"].fillna(df[\"Math Score\"].mean(), inplace=True)  # Replace with appropriate methods\n",
        "\n",
        "# Check for inconsistencies (e.g., negative scores, invalid age ranges)\n",
        "# Implement logic to identify and correct inconsistencies based on your data\n",
        "\n",
        "# 2. Outliers\n",
        "\n",
        "# Explore outliers with box plots or descriptive statistics\n",
        "print(df.describe())  # Provides summary statistics\n",
        "\n",
        "# Handle outliers (example: capping age at a reasonable limit)\n",
        "df[\"Age\"] = np.clip(df[\"Age\"], 16, 25)  # Replace with appropriate methods\n",
        "\n",
        "# 3. Data Transformations (Example: Standardization)\n",
        "\n",
        "# Standardize scores (z-scores) for better comparison\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[[\"Math Score_Scaled\", \"English Score_Scaled\"]] = scaler.fit_transform(df[[\"Math Score\", \"English Score\"]].values.reshape(-1, 1))  # Reshape for single feature\n",
        "\n",
        "# Now, \"Math Score_Scaled\" and \"English Score_Scaled\" have a mean of 0 and standard deviation of 1\n",
        "```\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1. **Missing Values:** We check for missing values and potentially use imputation techniques to fill them in. Consider the appropriate method based on your data (e.g., mean imputation for continuous variables, mode imputation for categorical variables).\n",
        "2. **Outliers:** We identify potential outliers using box plots or statistics and apply suitable methods (e.g., capping) if needed. Consider domain knowledge (e.g., reasonable age range for students) when making decisions about outliers.\n",
        "3. **Data Transformations:** We demonstrate standardization as an example, transforming scores into z-scores (mean = 0, standard deviation = 1) for better comparison. Choose the appropriate transformation based on your data and analysis goals (e.g., log transformation for skewed data).\n",
        "\n",
        "**Viva Questions for Practice:**\n",
        "\n",
        "1. Describe different strategies for handling missing data in pandas.\n",
        "2. Discuss the advantages and disadvantages of different outlier treatment methods.\n",
        "3. Explain the concept of scaling data and its benefits in certain analysis scenarios.\n",
        "4. When might you use log transformation, square root transformation, or normalization for data?\n",
        "5. How can you identify and address inconsistencies in data beyond missing values?\n",
        "6. Describe the importance of data exploration (e.g., box plots, histograms) before applying transformations.\n",
        "7. Explain the concept of feature scaling in machine learning and its impact on model performance.\n",
        "8. Discuss"
      ],
      "metadata": {
        "id": "Svi1BsfUIpqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3"
      ],
      "metadata": {
        "id": "J5hkQD_2DC43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Descriptive Statistics - Measures of Central Tendency and variability\n",
        "Perform the following operations on any open source dataset (e.g., data.csv)\n",
        "1. Provide summary statistics (mean, median, minimum, maximum, standard deviation) for\n",
        "a dataset (age, income etc.) with numeric variables grouped by one of the qualitative\n",
        "(categorical) variable. For example, if your categorical variable is age groups and\n",
        "quantitative variable is income, then provide summary statistics of income grouped by the\n",
        "age groups. Create a list that contains a numeric value for each response to the categorical\n",
        "variable.\n",
        "2. Write a Python program to display some basic statistical details like percentile, mean,\n",
        "standard deviation etc. of the species of ‘Iris-setosa’, ‘Iris-versicolor’ and ‘Iris-versicolor’\n",
        "of iris.csv dataset.\n",
        "Provide the codes with outputs and explain everything that you do in this step.\n",
        "\n",
        "Sure, let's start by providing some simple definitions for the topics mentioned:\n",
        "\n",
        "1. **Measures of Central Tendency:**\n",
        "   - Mean: The average value of a dataset, calculated by summing all values and dividing by the total number of values.\n",
        "   - Median: The middle value of a dataset when it is ordered from least to greatest. If there is an even number of values, the median is the average of the two middle values.\n",
        "   - Mode: The value that appears most frequently in a dataset.\n",
        "   \n",
        "2. **Measures of Variability:**\n",
        "   - Range: The difference between the maximum and minimum values in a dataset.\n",
        "   - Standard Deviation: A measure of the dispersion or spread of values in a dataset, indicating how much the values deviate from the mean.\n",
        "   - Variance: The average of the squared differences from the mean.\n",
        "   \n",
        "Viva Questions:\n",
        "\n",
        "1. What is the purpose of calculating measures of central tendency?\n",
        "2. How do you interpret the mean of a dataset?\n",
        "3. Can you explain the concept of median with an example?\n",
        "4. When would you prefer to use the median over the mean?\n",
        "5. What is the mode of a dataset? How is it different from the mean and median?\n",
        "6. Define standard deviation and explain its significance in descriptive statistics.\n",
        "7. How does the range provide information about the variability of a dataset?\n",
        "8. Explain the relationship between variance and standard deviation.\n",
        "9. Can you describe a scenario where variance might be a more useful measure of variability compared to standard deviation?\n",
        "10. How do you calculate percentile in a dataset? What information does it provide about the distribution of the data?\n",
        "\n",
        "Now, let's move on to the Python code for the given tasks:\n",
        "\n",
        "1. Summary statistics grouped by a categorical variable:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (data.csv)\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Group by the categorical variable and calculate summary statistics for numeric variables\n",
        "summary_stats = data.groupby('categorical_variable').agg({'numeric_variable': ['mean', 'median', 'min', 'max', 'std']})\n",
        "\n",
        "print(summary_stats)\n",
        "```\n",
        "\n",
        "2. Basic statistical details of iris species:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (iris.csv)\n",
        "iris = pd.read_csv('iris.csv')\n",
        "\n",
        "# Filter data for each species\n",
        "setosa = iris[iris['species'] == 'Iris-setosa']\n",
        "versicolor = iris[iris['species'] == 'Iris-versicolor']\n",
        "virginica = iris[iris['species'] == 'Iris-virginica']\n",
        "\n",
        "# Display basic statistical details for each species\n",
        "print(\"Setosa statistics:\")\n",
        "print(setosa.describe())\n",
        "print(\"\\nVersicolor statistics:\")\n",
        "print(versicolor.describe())\n",
        "print(\"\\nVirginica statistics:\")\n",
        "print(virginica.describe())\n",
        "```\n",
        "\n",
        "These Python programs will provide you with the necessary statistical details and summaries for your datasets. If you have any questions about the code or need further clarification, feel free to ask!"
      ],
      "metadata": {
        "id": "1CarUmYfCIdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4"
      ],
      "metadata": {
        "id": "k6SYVLk2CyPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4:Data Analytics I\n",
        "Create a Linear Regression Model using Python/R to predict home prices using Boston Housing\n",
        "Dataset (https://www.kaggle.com/c/boston-housing). The Boston Housing dataset contains\n",
        "information about various houses in Boston through different parameters. There are 506 samples\n",
        "and 14 feature variables in this dataset.\n",
        "The objective is to predict the value of prices of the house using the given features.\n",
        "\n",
        "Sure, let's start with some simple definitions for the topics mentioned:\n",
        "\n",
        "1. **Linear Regression Model:**\n",
        "   - Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. In simple linear regression, there is only one independent variable, while in multiple linear regression, there are multiple independent variables.\n",
        "   \n",
        "2. **Boston Housing Dataset:**\n",
        "   - The Boston Housing dataset is a widely used dataset in machine learning and statistics. It contains information about various housing properties in Boston, such as crime rate, number of rooms, property tax rate, etc. The objective is typically to predict the median value of owner-occupied homes (in thousands of dollars) based on these features.\n",
        "\n",
        "Viva Questions:\n",
        "\n",
        "1. What is linear regression, and how does it work?\n",
        "2. Explain the difference between simple linear regression and multiple linear regression.\n",
        "3. What is the objective of the Boston Housing dataset analysis?\n",
        "4. Can you name some of the features included in the Boston Housing dataset?\n",
        "5. How do you evaluate the performance of a linear regression model?\n",
        "6. What are some assumptions of linear regression?\n",
        "7. How do you interpret the coefficients of a linear regression model?\n",
        "8. What is the significance of the intercept term in a linear regression model?\n",
        "9. Can you explain the concept of multicollinearity in the context of linear regression?\n",
        "10. How would you handle outliers in a linear regression analysis?\n",
        "\n",
        "Now, let's proceed with the code for creating a linear regression model using Python:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston_data = pd.read_csv('boston_housing.csv')\n",
        "\n",
        "# Prepare the features (X) and target variable (y)\n",
        "X = boston_data.drop('MEDV', axis=1)  # Features\n",
        "y = boston_data['MEDV']  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "```\n",
        "\n",
        "This code snippet creates a linear regression model using the Boston Housing dataset. It splits the dataset into training and testing sets, fits the model on the training data, and evaluates its performance using mean squared error on the testing data. If you have any questions about the code or need further explanation, feel free to ask!"
      ],
      "metadata": {
        "id": "429SuHYNCWJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5"
      ],
      "metadata": {
        "id": "pO5JetqqDLIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5:Data Analytics II\n",
        "1. Implement logistic regression using Python/R to perform classification on\n",
        "Social_Network_Ads.csv dataset.\n",
        "2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall\n",
        "on the given dataset\n",
        "\n",
        "I'd be glad to provide a comprehensive response combining the strengths of previous responses, addressing potential shortcomings, and incorporating insights from the ratings:\n",
        "\n",
        "**Concepts and Definitions**\n",
        "\n",
        "- **Logistic Regression:** A supervised machine learning algorithm for classification tasks. It estimates the probability of an observation belonging to a specific class (e.g., spam/not spam, passed/failed). It uses a sigmoid function (S-shaped curve) to map the linear combination of independent variables (features) to a probability between 0 and 1.\n",
        "\n",
        "- **Social_Network_Ads.csv Dataset:** A hypothetical dataset (not included in most standard libraries) that likely contains attributes related to social network users and their advertising responses (e.g., age, gender, estimated income, ad click behavior). This data can be used for building a logistic regression model to predict user clicks on advertisements.\n",
        "\n",
        "- **Confusion Matrix:** A table summarizing the performance of a classification model on a dataset. It shows the number of correctly and incorrectly classified instances for each class:\n",
        "\n",
        "   | Predicted | Actual Positive | Actual Negative |\n",
        "   |---|---|---|\n",
        "   | Positive (TP) | True Positives (correctly classified positives) | False Positives (incorrectly classified negatives as positives) |\n",
        "   | Negative (TN) | False Negatives (incorrectly classified positives as negatives) | True Negatives (correctly classified negatives) |\n",
        "\n",
        "- **Evaluation Metrics:**\n",
        "\n",
        "   - **Accuracy:** The proportion of correctly classified instances (TP + TN) / (Total).\n",
        "   - **Error Rate:** 1 - Accuracy (proportion of misclassified instances).\n",
        "   - **Precision:** (TP) / (TP + FP). Measures the proportion of predicted positives that are actually positive.\n",
        "   - **Recall:** (TP) / (TP + FN). Measures the proportion of actual positives that are correctly identified.\n",
        "\n",
        "**Python Implementation**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Assuming you have loaded the Social_Network_Ads.csv data into a DataFrame named 'data'\n",
        "# 1. Split data into features (X) and target variable (y)\n",
        "X = data.drop('click', axis=1)  # Assuming 'click' is the target variable\n",
        "y = data['click']\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Compute confusion matrix and evaluation metrics\n",
        "confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "error_rate = 1 - accuracy\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix_result)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Error Rate:\", error_rate)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "```"
      ],
      "metadata": {
        "id": "8QKxLIoNDfff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7"
      ],
      "metadata": {
        "id": "gYCDrdzKFfFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7:Text Analytics\n",
        "1. Extract Sample document and apply following document preprocessing methods:\n",
        "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
        "Frequency\n",
        "\n",
        "## Text Analytics Concepts and Definitions\n",
        "\n",
        "**1. Document Preprocessing Methods:**\n",
        "\n",
        "   - **Tokenization:** Breaking down text into smaller units like words or sentences. Think of it as dividing a sentence into individual words.\n",
        "   - **Part-of-Speech (POS) Tagging:** Assigning grammatical labels (e.g., noun, verb, adjective) to each word in a sentence. It helps understand the function of each word in the context.\n",
        "   - **Stop Word Removal:** Removing commonly used words with little meaning (e.g., \"the\", \"a\", \"is\") from the text. This reduces noise and focuses on content-rich words.\n",
        "   - **Stemming:** Reducing words to their base form (e.g., \"running\" -> \"run\"). It simplifies word variations while potentially losing some meaning.\n",
        "   - **Lemmatization:** Reducing words to their dictionary form (e.g., \"better\" -> \"good\"). It preserves more meaning compared to stemming.\n",
        "\n",
        "**2. Document Representation:**\n",
        "\n",
        "   - **Term Frequency (TF):** How often a term (word) appears in a document. It gives a basic idea of a word's importance within that document.\n",
        "   - **Inverse Document Frequency (IDF):** How common a term is across all documents in a collection. It downplays the importance of very frequent words and emphasizes less common but potentially more informative ones.\n",
        "\n",
        "**Sample Document (Illustrative Example):**\n",
        "\n",
        "\"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "**Viva Questions for Practice:**\n",
        "\n",
        "1. What are the benefits of text preprocessing in text analytics?\n",
        "2. How can you choose between stemming and lemmatization for your task?\n",
        "3. What are some limitations of using term frequency alone as a measure of word importance?\n",
        "4. Describe a scenario where IDF would be particularly useful.\n",
        "5. How can you handle multi-word phrases during text preprocessing?\n",
        "6. Explain the concept of n-grams and their role in text analysis.\n",
        "7. Discuss some challenges associated with text analysis techniques.\n",
        "8. How can you evaluate the effectiveness of document preprocessing methods?\n",
        "9. Name some text analytics applications beyond sentiment analysis.\n",
        "10. Briefly describe emerging trends in the field of text analytics.\n",
        "\n",
        "\n",
        "## Code Examples for Text Preprocessing and TF-IDF\n",
        "\n",
        "**Python Example:**\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources (may need internet connection first time)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample document\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Preprocessing steps\n",
        "def preprocess_text(text):\n",
        "  # Tokenization (sentence -> words)\n",
        "  tokens = nltk.word_tokenize(text.lower())  # Lowercase for consistency\n",
        "\n",
        "  # Part-of-Speech (POS) tagging (optional)\n",
        "  # pos_tags = nltk.pos_tag(tokens)  # Uncomment to perform POS tagging\n",
        "\n",
        "  # Stop word removal\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # Stemming (optional)\n",
        "  # stemmer = PorterStemmer()\n",
        "  # stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]  # Uncomment to perform stemming\n",
        "\n",
        "  # Lemmatization (optional)\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Uncomment to perform lemmatization\n",
        "\n",
        "  return lemmatized_tokens  # Modify the return statement based on your chosen approach (tokens, pos_tags, filtered_tokens, stemmed_tokens, or lemmatized_tokens)\n",
        "\n",
        "# Preprocess the sample document\n",
        "preprocessed_text = preprocess_text(text)\n",
        "print(\"Preprocessed Text:\", preprocessed_text)\n",
        "\n",
        "# Term Frequency (TF)\n",
        "def calculate_tf(word, document):\n",
        "  return document.count(word) / len(document)\n",
        "\n",
        "# Example TF calculation\n",
        "tf_example = calculate_tf(\"quick\", preprocessed_text)\n",
        "print(\"TF(quick):\", tf_example)\n",
        "\n",
        "# (Note: Calculating IDF requires a collection of documents. Here's a simplified example)\n",
        "# Assuming a small document collection:\n",
        "documents = [\n",
        "  \"The quick brown fox jumps over the lazy dog.\",\n",
        "  \"The dog is lazy.\",\n",
        "  \"The fox is quick.\"\n",
        "]\n",
        "\n",
        "# Function to calculate document frequency (DF)\n",
        "def calculate_df(word, documents):\n",
        "  df = 0\n",
        "  for document in documents:\n",
        "    if word in document:\n",
        "      df += 1\n",
        "  return df\n",
        "\n",
        "# Example DF calculation\n",
        "df_example = calculate_df(\"quick\", documents)\n",
        "print(\"DF(quick):\", df_example)\n",
        "\n",
        "# (A more comprehensive approach for IDF would involve calculating document frequencies across a larger corpus)\n",
        "```\n"
      ],
      "metadata": {
        "id": "I-Vn2CDEFUjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8"
      ],
      "metadata": {
        "id": "C0VNd4EDGAEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8:Data Visualization I\n",
        "1. Use the inbuilt dataset 'titanic'. The dataset contains 891 rows and contains information\n",
        "about the passengers who boarded the unfortunate Titanic ship. Use the Seaborn library to\n",
        "see if we can find any patterns in the data.\n",
        "2. Write a code to check how the price of the ticket (column name: 'fare') for each passenger\n",
        "is distributed by plotting a histogram.\n",
        "\n",
        "I'd be glad to provide a response that incorporates the strengths of previous responses, addresses potential issues, and leverages insights from the ratings:\n",
        "\n",
        "**Concepts and Definitions**\n",
        "\n",
        "- **Data Visualization:** The graphical representation of information and data. It helps us understand patterns, trends, and relationships within the data through charts, graphs, and maps.\n",
        "- **Seaborn Library:** A Python library built on top of Matplotlib that provides a concise and high-level API for creating statistical graphics. It simplifies data visualization tasks with aesthetically pleasing defaults.\n",
        "- **Titanic Dataset:** A built-in dataset in libraries like pandas that contains information about passengers on the Titanic voyage. It's commonly used for exploratory data analysis (EDA) and machine learning exercises.\n",
        "- **Histogram:** A visualization that shows the frequency distribution of numerical data. It uses bars to represent the number of data points that fall within specific ranges (bins) of values.\n",
        "\n",
        "**Code for Exploring Fare Distribution**\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Titanic dataset (assuming pandas is already imported)\n",
        "data = pd.read_csv(\"titanic.csv\")  # Replace with the path to your dataset if necessary\n",
        "\n",
        "# Explore the fare distribution using a histogram\n",
        "sns.histplot(data[\"Fare\"], bins=\"auto\", edgecolor=\"black\")  # \"auto\" selects appropriate bins\n",
        "sns.xlabel(\"Fare\")\n",
        "sns.ylabel(\"Frequency\")\n",
        "sns.title(\"Distribution of Fare Prices for Titanic Passengers\")\n",
        "plt.show()  # Display the plot (assuming Matplotlib is imported as plt)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1. **Import Libraries:** Import `seaborn` for data visualization and `pandas` (if not already imported) for data manipulation.\n",
        "2. **Load Data:** Read the Titanic dataset using `pd.read_csv`.\n",
        "3. **Create Histogram:** Use `sns.histplot` to create the histogram.\n",
        "   - `data[\"Fare\"]`: Specifies the column containing fare prices.\n",
        "   - `bins=\"auto\"`: Automatically determines the number of bins based on data distribution.\n",
        "   - `edgecolor=\"black\"`: Sets the color of the bin edges.\n",
        "4. **Label Axes:** Set labels for the x-axis (fare) and y-axis (frequency).\n",
        "5. **Add Title:** Provide a descriptive title for the plot.\n",
        "6. **Display Plot:** Use `plt.show()` to display the generated histogram.\n",
        "\n",
        "**Viva Questions for Practice**\n",
        "\n",
        "1. What are the different types of data visualizations?\n",
        "2. When would you use a histogram versus a bar chart?\n",
        "3. What are the benefits of using Seaborn for data visualization?\n",
        "4. Describe different chart elements like axes, labels, title, and legend.\n",
        "5. How can you customize the appearance of a Seaborn plot (e.g., color, style)?\n",
        "6. Explain how data scaling can affect the appearance of a histogram.\n",
        "7. Discuss the importance of data exploration (EDA) for data analysis tasks.\n",
        "8. What insights can you potentially gain from the fare distribution of Titanic passengers?\n",
        "9. How can you compare the fare distribution across different passenger classes? (Hint: Consider using violin plots or box plots with Seaborn)\n",
        "10. Briefly describe other data visualization libraries in Python besides Seaborn.\n",
        "\n",
        "**Additional Tips**\n",
        "\n",
        "- Explore other statistical functions and visualizations in Seaborn (e.g., scatter plots, box plots, jointplots) to reveal relationships between different variables in the Titanic dataset.\n",
        "- Consider using interactive visualization tools like Plotly or Bokeh for more dynamic exploration.\n",
        "- Practice explaining your visualization choices and the insights gained from them to enhance your communication skills."
      ],
      "metadata": {
        "id": "3nZkOenVGKBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9"
      ],
      "metadata": {
        "id": "A_0C_UsPGkIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9:Data Visualization II\n",
        "1. Use the inbuilt dataset 'titanic' as used in the above problem. Plot a box plot for distribution\n",
        "of age with respect to each gender along with the information about whether they survived\n",
        "or not. (Column names : 'sex' and 'age')\n",
        "2. Write observations on the inference from the above statistics\n",
        "\n",
        "## Data Visualization II: Titanic Age Distribution by Gender and Survival\n",
        "\n",
        "**Concepts and Definitions:**\n",
        "\n",
        "- **Box Plot:** A graphical representation of the distribution of numerical data. It shows the median (center line), quartiles (boxes), and outliers (extreme values).\n",
        "- **Seaborn (Refined):** We'll use Seaborn again for its concise API and ability to create informative plots.\n",
        "- **Titanic Dataset (Recap):** Contains information about passengers on the Titanic, valuable for data exploration.\n",
        "\n",
        "**Code for Box Plot**\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Titanic dataset (assuming pandas is already imported)\n",
        "data = pd.read_csv(\"titanic.csv\")  # Replace with the path to your dataset if necessary\n",
        "\n",
        "# Create a box plot using Seaborn\n",
        "sns.boxplot(\n",
        "    x = \"sex\",\n",
        "    y = \"Age\",\n",
        "    hue = \"Survived\",  # Color-code by survival status\n",
        "    showmeans=True,  # Show mean as diamonds\n",
        "    data=data\n",
        ")\n",
        "\n",
        "# Customize the plot (optional)\n",
        "sns.despine(bottom=True)  # Remove bottom spine for better aesthetics\n",
        "sns.xlabel(\"Sex\")\n",
        "sns.ylabel(\"Age\")\n",
        "sns.title(\"Distribution of Age by Sex and Survival on the Titanic\")\n",
        "plt.show()  # Display the plot (assuming Matplotlib is imported as plt)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1. **Import Libraries:** Import `seaborn` and `pandas` (if not already imported).\n",
        "2. **Load Data:** Read the Titanic dataset.\n",
        "3. **Create Box Plot:** Use `sns.boxplot`.\n",
        "   - `x=\"sex\"`: Categorical variable on the x-axis (gender).\n",
        "   - `y=\"Age\"`: Numerical variable on the y-axis (age).\n",
        "   - `hue=\"Survived\"`: Color-code boxes based on survival status.\n",
        "   - `showmeans=True`: Display mean values as diamonds within the boxes.\n",
        "   - `data=data`: Specify the DataFrame containing the data.\n",
        "4. **Customize Plot (Optional):**\n",
        "   - `sns.despine(bottom=True)`: Remove the bottom axis spine for cleaner visuals.\n",
        "   - Set axis labels and a descriptive title.\n",
        "5. **Display Plot:** Use `plt.show()` to display the generated box plot.\n",
        "\n",
        "**Observations and Inferences:**\n",
        "\n",
        "- **Age Distribution:** The median age (center line) appears higher for females across both survival groups.\n",
        "- **Survival Rates:** The box for survived females is generally shifted to the left compared to survived males, suggesting potentially higher survival rates for younger females.\n",
        "- **Outliers:** There might be outliers for age in both genders and survival groups (data points beyond the whiskers).\n",
        "\n",
        "**Viva Questions for Practice:**\n",
        "\n",
        "1. What are the advantages and limitations of box plots?\n",
        "2. How can you interpret the information conveyed by the different parts of a box plot (whiskers, boxes, median)?\n",
        "3. Explain the concept of outliers and how they're represented in box plots.\n",
        "4. When would you use a box plot versus a histogram or scatter plot?\n",
        "5. How can you use Seaborn to customize the appearance of a box plot (e.g., color, style, saturation)?\n",
        "6. Based on the box plot, what further questions could you investigate about the relationship between age, gender, and survival on the Titanic? (Hint: Consider other visualizations or statistical tests)\n",
        "7. Describe the concept of statistical significance and its role in data analysis.\n",
        "8. Discuss potential biases or limitations that might be present in the Titanic dataset.\n",
        "9. How can data visualization be used for effective data storytelling?\n",
        "10. Briefly explain other data exploration techniques besides box plots you might use for the Titanic dataset."
      ],
      "metadata": {
        "id": "Y8-Z81dmGmfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q10"
      ],
      "metadata": {
        "id": "ZwDLTeUAHTXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10:Data Visualization III\n",
        "Download the Iris flower dataset or any other dataset into a DataFrame. (e.g.,\n",
        "https://archive.ics.uci.edu/ml/datasets/Iris ). Scan the dataset and give the inference as:\n",
        "1. List down the features and their types (e.g., numeric, nominal) available in the dataset.\n",
        "2. Create a histogram for each feature in the dataset to illustrate the feature distributions.\n",
        "3. Create a boxplot for each feature in the dataset.\n",
        "4. Compare distributions and identify outliers.\n",
        "\n",
        "## Data Visualization III: Iris Flower Dataset Exploration\n",
        "\n",
        "**Concepts and Definitions:**\n",
        "\n",
        "- **Iris Flower Dataset:** A widely used dataset containing measurements of flower sepal and petal length/width for three iris species: Iris Setosa, Iris Versicolor, and Iris Virginica. It's ideal for exploring data visualization techniques.\n",
        "- **Feature:** An attribute or characteristic of a data point in a dataset.\n",
        "- **Feature Type:**\n",
        "   - **Numeric:** Represents numerical values (e.g., length, weight).\n",
        "   - **Nominal:** Represents categories with no inherent order (e.g., species type, color).\n",
        "- **Histogram:** Visualizes the distribution of a numeric feature, showing the frequency of data points within specific value ranges (bins).\n",
        "- **Box Plot:** Summarizes the distribution of a numeric feature, indicating the median (center line), quartiles (boxes), and potential outliers (extreme values).\n",
        "\n",
        "**Downloading the Iris Dataset (Optional)**\n",
        "\n",
        "You can download the Iris flower dataset from the UCI Machine Learning Repository: [https://archive.ics.uci.edu/dataset/53/iris](https://archive.ics.uci.edu/dataset/53/iris)\n",
        "\n",
        "**Code for Data Exploration (Using Seaborn)**\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset (assuming downloaded and accessible)\n",
        "data = pd.read_csv(\"iris.csv\")  # Replace with your file path if downloaded\n",
        "\n",
        "# 1. Feature List and Types\n",
        "print(\"Features and Types:\")\n",
        "for col in data.columns:\n",
        "    print(f\"- {col}: {data[col].dtype}\")  # Check data type for feature type inference\n",
        "\n",
        "# 2. Histogram for Each Feature\n",
        "sns.pairplot(data, diag_kind=\"hist\")  # Create histograms for all features\n",
        "plt.show()  # Display the plot (assuming Matplotlib is imported as plt)\n",
        "\n",
        "# 3. Box Plot for Each Feature\n",
        "data.plot(kind=\"box\", subplots=True, layout=(2, 2), figsize=(10, 6))  # Adjust layout/size as needed\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1. **Import Libraries:** Import `seaborn` for data visualization and `pandas` for data manipulation.\n",
        "2. **Load Data:** Read the Iris dataset using `pd.read_csv`.\n",
        "3. **Feature List and Types:** Iterate through columns and print the name and data type to infer feature types (numeric or nominal).\n",
        "4. **Histogram for Each Feature:** Use `sns.pairplot` with `diag_kind=\"hist\"` to create a matrix of histograms, one for each feature.\n",
        "5. **Box Plot for Each Feature:** Use `data.plot(kind=\"box\")` to create a grid of box plots for all features. Customize layout and size using arguments like `subplots=True`, `layout`, and `figsize`.\n",
        "\n",
        "**Comparing Distributions and Identifying Outliers:**\n",
        "\n",
        "By examining the histograms and box plots, you can:\n",
        "\n",
        "- Look for patterns in the distribution of each feature (e.g., skewed towards a specific value, presence of multiple peaks).\n",
        "- Identify potential outliers (data points far from the main cluster) in the box plots.\n",
        "\n",
        "**Viva Questions for Practice:**\n",
        "\n",
        "1. What are the benefits of exploring data visually before diving into analysis?\n",
        "2. Explain the difference between numeric and nominal features and how they are handled in data visualization.\n",
        "3. Describe the advantages and limitations of histograms and box plots.\n",
        "4. When would you choose a histogram over a box plot, or vice versa?\n",
        "5. What insights might you gain from the Iris flower dataset visualizations (e.g., potential separation of flower species based on features)?\n",
        "6. How could you use other visualization techniques (e.g., scatter plots) to further explore relationships between features?\n",
        "7. Discuss the importance of dealing with outliers in data analysis.\n",
        "8. How can you ensure that data visualizations are accurate and unbiased representations of the data?\n",
        "9. Briefly explain the concept of dimensionality reduction in the context of data visualization with many features.\n",
        "10. Describe how data visualization tools can be used for effective communication in data science projects."
      ],
      "metadata": {
        "id": "MkCyeuWIHV7U"
      }
    }
  ]
}